# åç§»é‡æ–¹æ³•è¯¦è§£ï¼šä»é¢„æµ‹åˆ°å®ä¾‹

## ä¸€ã€æ ¸å¿ƒé—®é¢˜è§£ç­”

### 1ï¸âƒ£ åç§»é‡å¦‚ä½•è½¬æ¢ä¸ºå®ä¾‹ï¼Ÿ

**æ ¸å¿ƒæ€æƒ³**ï¼šé¢„æµ‹çš„åç§»é‡æŒ‡å‘å®ä¾‹ä¸­å¿ƒï¼Œå°†ç‚¹äº‘åæ ‡åŠ ä¸Šåç§»é‡ï¼Œå¾—åˆ°çš„"é¢„æµ‹ä¸­å¿ƒ"åœ¨ç©ºé—´ä¸­èšç±»ï¼ŒåŒä¸€èšç±»çš„ç‚¹å°±å±äºåŒä¸€å®ä¾‹ã€‚

```
ç‚¹äº‘åæ ‡ + åç§»é‡ â†’ é¢„æµ‹ä¸­å¿ƒ â†’ HDBSCANèšç±» â†’ å®ä¾‹ID
```

### 2ï¸âƒ£ è®­ç»ƒæ—¶å¦‚ä½•å­¦ä¹ åç§»é‡ï¼Ÿ

**æ ¸å¿ƒæ€æƒ³**ï¼šGround Truthæä¾›æ¯ä¸ªç‚¹çš„çœŸå®å®ä¾‹æ ‡ç­¾ï¼Œè®¡ç®—æ¯ä¸ªå®ä¾‹çš„çœŸå®ä¸­å¿ƒï¼Œè®­ç»ƒç›®æ ‡æ˜¯è®©æ¯ä¸ªç‚¹é¢„æµ‹çš„åç§»å‘é‡æŒ‡å‘è¯¥å®ä¾‹ä¸­å¿ƒã€‚

```
è®­ç»ƒç›®æ ‡ï¼šp_i + offset_pred_i â‰ˆ center_gt (è¯¥ç‚¹æ‰€å±å®ä¾‹çš„çœŸå®ä¸­å¿ƒ)
```

### 3ï¸âƒ£ é¢„æµ‹å’Œæ ‡æ³¨å¦‚ä½•å¯¹åº”ï¼Ÿ

**æ•°æ®å¯¹åº”å…³ç³»**ï¼š
- è¾“å…¥ï¼šç‚¹äº‘åæ ‡ (positions)
- æ ‡æ³¨ï¼šinstance æ ‡ç­¾ï¼Œå‘Šè¯‰æ¯ä¸ªç‚¹å±äºå“ªä¸ªå®ä¾‹
- è®­ç»ƒç›®æ ‡ï¼šæ ¹æ® instance æ ‡ç­¾è®¡ç®—çœŸå®ä¸­å¿ƒï¼Œè®©æ¨¡å‹é¢„æµ‹çš„åç§»æŒ‡å‘å®ƒ
- æ¨ç†æ—¶ï¼šæ²¡æœ‰ instance æ ‡ç­¾ï¼Œæ¨¡å‹é¢„æµ‹åç§»åèšç±»å¾—åˆ°å®ä¾‹

---

## äºŒã€å®Œæ•´æµç¨‹å›¾è§£

### ğŸ“Š è®­ç»ƒæµç¨‹ï¼ˆTrainingï¼‰

```
è¾“å…¥æ•°æ® (Ground Truth)
â”œâ”€â”€ positions      [N, 3]    ç‚¹äº‘åæ ‡
â”œâ”€â”€ colors         [N, 3]    RGBé¢œè‰²
â”œâ”€â”€ semantic       [N, 1]    è¯­ä¹‰æ ‡ç­¾ï¼ˆfruit=3, trunk=4ï¼‰
â”œâ”€â”€ instance       [N, 1]    æ ‡å‡†å®ä¾‹IDï¼ˆæ¯ä¸ªæœå®ã€æ ‘å¹²ä¸€ä¸ªIDï¼‰
â””â”€â”€ instance_h     [N, 1]    æ ‘æœ¨å®ä¾‹IDï¼ˆæ¯æ£µæ ‘ä¸€ä¸ªIDï¼‰

           â†“
    ã€åˆ›å»º TensorFieldã€‘
           â†“
    ã€æ¨¡å‹å‰å‘ä¼ æ’­ã€‘
           â†“
æ¨¡å‹è¾“å‡º (Predictions)
â”œâ”€â”€ output_sem     TensorField  è¯­ä¹‰logits [N, 6]
â”œâ”€â”€ offsets1       TensorField  æ ‡å‡†å®ä¾‹åç§»å‘é‡ [N, D]
â””â”€â”€ offsets2       TensorField  æ ‘æœ¨å®ä¾‹åç§»å‘é‡ [N, D]

           â†“
    ã€è®¡ç®—æŸå¤±å‡½æ•°ã€‘
           â†“
æŸå¤±è®¡ç®—
â”œâ”€â”€ L_semantic     : CrossEntropyLoss(é¢„æµ‹è¯­ä¹‰, GTè¯­ä¹‰)
â”œâ”€â”€ L_instance1    : IoULovaszLoss(offsets1, GT_instance)
â”œâ”€â”€ L_instance2    : IoULovaszLoss(offsets2, GT_instance_h)
â””â”€â”€ L_HCL          : HierarchicalConsistencyLoss(offsets1, offsets2)

           â†“
    ã€åå‘ä¼ æ’­ä¼˜åŒ–ã€‘
```

### ğŸ“Š æ¨ç†æµç¨‹ï¼ˆInferenceï¼‰

```
è¾“å…¥æ•°æ® (æ— æ ‡æ³¨)
â”œâ”€â”€ positions      [N, 3]    ç‚¹äº‘åæ ‡
â””â”€â”€ colors         [N, 3]    RGBé¢œè‰²

           â†“
    ã€åˆ›å»º TensorFieldã€‘
           â†“
    ã€æ¨¡å‹å‰å‘ä¼ æ’­ã€‘
           â†“
æ¨¡å‹è¾“å‡º
â”œâ”€â”€ output_sem     TensorField  è¯­ä¹‰logits [N, 6]
â”œâ”€â”€ offsets1       TensorField  æ ‡å‡†å®ä¾‹åç§»å‘é‡ [N, D]
â””â”€â”€ offsets2       TensorField  æ ‘æœ¨å®ä¾‹åç§»å‘é‡ [N, D]

           â†“
    ã€æå–ç‰¹å¾ã€‘
sem_pred = argmax(output_sem.features_at(0))  # [N] è¯­ä¹‰é¢„æµ‹
offsets_ins = offsets1.features_at(0)[:, :3]  # [N, 3] å®ä¾‹åç§»xyz
offsets_tree = offsets2.features_at(0)[:, :3] # [N, 3] æ ‘æœ¨åç§»xyz

           â†“
    ã€è®¡ç®—é¢„æµ‹ä¸­å¿ƒã€‘
coords = positions  # [N, 3]
centers_ins = coords + offsets_ins    # [N, 3] æ ‡å‡†å®ä¾‹é¢„æµ‹ä¸­å¿ƒ
centers_tree = coords + offsets_tree  # [N, 3] æ ‘æœ¨å®ä¾‹é¢„æµ‹ä¸­å¿ƒ

           â†“
    ã€HDBSCANèšç±»ã€‘
ins_pred = HDBSCAN(centers_ins)       # [N] æ ‡å‡†å®ä¾‹ID
ins_h_pred = HDBSCAN(centers_tree)    # [N] æ ‘æœ¨å®ä¾‹ID

           â†“
è¾“å‡ºé¢„æµ‹
â”œâ”€â”€ semantic       [N]    è¯­ä¹‰é¢„æµ‹
â”œâ”€â”€ instance       [N]    æ ‡å‡†å®ä¾‹é¢„æµ‹
â””â”€â”€ instance_h     [N]    æ ‘æœ¨å®ä¾‹é¢„æµ‹
```

---

## ä¸‰ã€æ ¸å¿ƒä»£ç è§£æ

### ğŸ”´ è®­ç»ƒé˜¶æ®µçš„æŸå¤±å‡½æ•°ï¼ˆutils/lovasz.pyï¼‰

```python
class IoULovaszLoss(nn.Module):
    """
    å®ä¾‹åˆ†å‰²æŸå¤±ï¼šåŸºäºåç§»é‡çš„IoU + LovaszæŸå¤±
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. è®¡ç®—æ¯ä¸ªGTå®ä¾‹çš„çœŸå®ä¸­å¿ƒ
    2. æ¯ä¸ªç‚¹é¢„æµ‹çš„åç§»åº”æŒ‡å‘å…¶æ‰€å±å®ä¾‹çš„ä¸­å¿ƒ
    3. ä½¿ç”¨varianceå‚æ•°æ§åˆ¶ä¸­å¿ƒé™„è¿‘çš„å®¹å¿åº¦
    """
    
    def forward(self, points, instance_labels, semantic_labels, offsets, voxel_resolution):
        # 1. è·å–åæ ‡å’Œåç§»
        coords = points.coordinates_at(0) * voxel_resolution  # [N, 3] åŸå§‹åæ ‡
        offsets_xyz = offsets.features_at(0)[:, :3]           # [N, 3] é¢„æµ‹åç§»
        
        # 2. è®¡ç®—é¢„æµ‹ä¸­å¿ƒ
        predicted_centers = coords + offsets_xyz  # [N, 3]
        
        # 3. æ ¹æ®GTå®ä¾‹æ ‡ç­¾è®¡ç®—æ¯ä¸ªå®ä¾‹çš„çœŸå®ä¸­å¿ƒ
        unique_instances = torch.unique(instance_labels[instance_labels > 0])
        
        gt_centers = {}
        for inst_id in unique_instances:
            mask = (instance_labels == inst_id)
            gt_centers[inst_id] = coords[mask].mean(dim=0)  # [3] è¯¥å®ä¾‹çš„çœŸå®ä¸­å¿ƒ
        
        # 4. è®¡ç®—æŸå¤±ï¼šé¢„æµ‹ä¸­å¿ƒä¸çœŸå®ä¸­å¿ƒçš„è·ç¦»
        loss = 0
        for point_idx, inst_id in enumerate(instance_labels):
            if inst_id <= 0:  # è·³è¿‡èƒŒæ™¯
                continue
            
            gt_center = gt_centers[inst_id]  # [3]
            pred_center = predicted_centers[point_idx]  # [3]
            
            # è·ç¦»æŸå¤± + IoUæŸå¤± + LovaszæŸå¤±
            dist = torch.norm(pred_center - gt_center)
            loss += dist
        
        return loss / len(instance_labels)
```

**å…³é”®ç‚¹è§£æ**ï¼š
- `coords + offsets_xyz`ï¼šç‚¹äº‘åæ ‡ + é¢„æµ‹åç§» = é¢„æµ‹ä¸­å¿ƒ
- `gt_centers[inst_id]`ï¼šGround Truthä¸­è¯¥å®ä¾‹çš„æ‰€æœ‰ç‚¹åæ ‡çš„å‡å€¼ = çœŸå®ä¸­å¿ƒ
- è®­ç»ƒç›®æ ‡ï¼šè®©é¢„æµ‹ä¸­å¿ƒé è¿‘çœŸå®ä¸­å¿ƒ

---

### ğŸŸ¢ æ¨ç†é˜¶æ®µçš„èšç±»ï¼ˆexport_ply.py å’Œ hapt3d_ours.pyï¼‰

**âš ï¸ é‡è¦è¯´æ˜**ï¼šHDBSCANèšç±»åå¤„ç†**åœ¨è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é˜¶æ®µéƒ½ä¼šæ‰§è¡Œ**

#### æ–¹å¼1ï¼šexport_ply.pyä¸­çš„æ‰‹åŠ¨å¯¼å‡º

```python
def model_inference(model, sample, voxel_resolution, device):
    # 1. æ¨¡å‹å‰å‘ä¼ æ’­
    output_sem, offsets1, offsets2 = model(dense_input)
    
    # 2. æå–ç‰¹å¾
    coords = dense_input.coordinates_at(0) * voxel_resolution  # [N, 3]
    offsets_ins = offsets1.features_at(0)[:, :3]               # [N, 3]
    offsets_tree = offsets2.features_at(0)[:, :3]              # [N, 3]
    
    # 3. è®¡ç®—é¢„æµ‹ä¸­å¿ƒ
    centers_ins = (coords + offsets_ins).cpu().numpy()   # [N, 3]
    centers_tree = (coords + offsets_tree).cpu().numpy() # [N, 3]
    
    # 4. HDBSCANèšç±»å¾—åˆ°å®ä¾‹ID
    clusterer_ins = HDBSCAN(min_cluster_size=50, min_samples=10)
    ins_pred = clusterer_ins.fit_predict(centers_ins)  # [N] å®ä¾‹ID
    
    clusterer_tree = HDBSCAN(min_cluster_size=200, min_samples=20)
    ins_h_pred = clusterer_tree.fit_predict(centers_tree)  # [N] æ ‘æœ¨å®ä¾‹ID
    
    return predictions
```

#### æ–¹å¼2ï¼šhapt3d_ours.pyä¸­çš„post_processingï¼ˆç”¨äºval/testï¼‰

```python
def post_processing(self, batch, output_sem, dense_input, offsets, hierarchy=False):
    """
    åœ¨validationå’Œtesté˜¶æ®µè°ƒç”¨ï¼Œå°†åç§»é‡è½¬æ¢ä¸ºå®ä¾‹ID
    
    è°ƒç”¨æ—¶æœº:
    - validation_step: å½“ epoch > pq_from_epoch æ—¶æ‰§è¡Œ
    - test_step: æ¯ä¸ªbatchéƒ½æ‰§è¡Œ
    """
    batch_size = len(batch["points"])
    ins_preds = []
    
    for batch_id in range(batch_size):
        # 1. è·å–åæ ‡å’Œè¯­ä¹‰é¢„æµ‹
        points_batch = dense_input.coordinates_at(batch_id) * self.voxel_resolution
        sem_pred_batch = torch.argmax(output_sem.features_at(batch_id), dim=1)
        offsets_batch = offsets.features_at(batch_id)
        
        # 2. è®¾ç½®thingsç±»åˆ«ID
        things_ids = [1] if hierarchy else THINGS_IDS  # [3, 4] for fruit/trunk
        
        # 3. å¯¹æ¯ä¸ªthingsç±»åˆ«è¿›è¡Œèšç±»
        ins_pred_batch = torch.zeros_like(sem_pred_batch)
        for things_id in things_ids:
            category_filter = (sem_pred_batch == things_id)
            
            # è®¡ç®—é¢„æµ‹ä¸­å¿ƒ (embeddings)
            embs_batch = points_batch[category_filter] + offsets_batch[category_filter]
            
            # HDBSCANèšç±»
            clustering = hdbscan_cpu(
                min_cluster_size=min_n_points,  # 50 for fruit/trunk, 200 for tree
                metric="minkowski",
                p=2.0
            ).fit(embs_batch.cpu().numpy())
            
            clusters = clustering.labels_  # [N_category] èšç±»ID (-1è¡¨ç¤ºå™ªå£°)
            
            # åˆ†é…å®ä¾‹IDï¼ˆä»å½“å‰æœ€å¤§ID+1å¼€å§‹ï¼‰
            ins_pred_batch[category_filter] += (clusters + 1 + ins_pred_batch.max())
        
        ins_preds.append(ins_pred_batch)
    
    return ins_preds

# åœ¨validation_stepä¸­è°ƒç”¨
def validation_step(self, batch, batch_idx):
    self.step(batch, step="val")

def step(self, batch, step, sensor='TLS'):
    # ... å‰å‘ä¼ æ’­ ...
    
    if step == "val":
        # è¯­ä¹‰åˆ†å‰²æŒ‡æ ‡
        preds = torch.argmax(logits, dim=1)
        self.jaccard(preds, sem_labels.squeeze())
        
        # å®ä¾‹åˆ†å‰²æŒ‡æ ‡ï¼ˆä»…åœ¨è¾¾åˆ°æŒ‡å®šepochåè®¡ç®—ï¼‰
        if self.trainer.current_epoch > self.pq_from_epoch:
            ins1_preds = self.post_processing(batch, output_sem, dense_input, offsets1)
            ins2_preds = self.post_processing(batch, output_sem, dense_input, offsets2, hierarchy=True)
            # ... è®¡ç®—PQæŒ‡æ ‡ ...
    
    if step == "test":
        # æµ‹è¯•é˜¶æ®µï¼šæ¯ä¸ªbatchéƒ½æ‰§è¡Œèšç±»
        ins1_preds = self.post_processing(batch, output_sem, dense_input, offsets1)
        ins2_preds = self.post_processing(batch, output_sem, dense_input, offsets2, hierarchy=True)
        # ... è®¡ç®—è¯„ä¼°æŒ‡æ ‡ ...
```

**å…³é”®ç‚¹è§£æ**ï¼š
- `centers_ins = coords + offsets_ins`ï¼šå°†åç§»é‡è½¬æ¢ä¸ºé¢„æµ‹ä¸­å¿ƒåæ ‡
- `HDBSCAN.fit_predict(centers_ins)`ï¼šå¯¹é¢„æµ‹ä¸­å¿ƒè¿›è¡Œèšç±»ï¼ŒåŒä¸€èšç±»=åŒä¸€å®ä¾‹
- èšç±»ç»“æœç›´æ¥ä½œä¸ºå®ä¾‹ID
- **è®­ç»ƒé˜¶æ®µä¸æ‰§è¡Œèšç±»**ï¼šåªè®¡ç®—æŸå¤±ï¼Œä½¿ç”¨GTæ ‡ç­¾ç›‘ç£
- **éªŒè¯é˜¶æ®µæœ‰æ¡ä»¶æ‰§è¡Œ**ï¼š`epoch > pq_from_epoch` æ—¶æ‰æ‰§è¡Œèšç±»è®¡ç®—PQ
- **æµ‹è¯•é˜¶æ®µå®Œå…¨æ‰§è¡Œ**ï¼šæ¯ä¸ªbatchéƒ½è¿›è¡Œèšç±»ï¼Œç”¨äºæœ€ç»ˆè¯„ä¼°

---

### ğŸŸ¡ å±‚æ¬¡ä¸€è‡´æ€§æŸå¤± HCLï¼ˆutils/hcl_loss.pyï¼‰

```python
class HierarchicalConsistencyLoss(nn.Module):
    """
    çº¦æŸå±‚æ¬¡å…³ç³»ï¼šåŒä¸€æ£µæ ‘çš„æœå®/æ ‘å¹²çš„å®ä¾‹ä¸­å¿ƒï¼Œåº”è¯¥é è¿‘è¯¥æ ‘çš„æ ‘æœ¨ä¸­å¿ƒ
    
    æ•°å­¦å½¢å¼ï¼š
    L_HCL = Î£_trees || mean(centers_ins_in_tree) - mean(centers_tree_in_tree) ||^2
    """
    
    def forward(self, coords, offset_inst, offset_tree, tree_labels, valid_mask):
        # 1. è®¡ç®—ä¸¤ç§é¢„æµ‹ä¸­å¿ƒ
        center_inst = coords + offset_inst  # [N, 3] æ ‡å‡†å®ä¾‹ä¸­å¿ƒ
        center_tree = coords + offset_tree  # [N, 3] æ ‘æœ¨å®ä¾‹ä¸­å¿ƒ
        
        # 2. å¯¹æ¯æ£µæ ‘è®¡ç®—ä¸€è‡´æ€§
        unique_trees = torch.unique(tree_labels[tree_labels > 0])
        
        total_loss = 0
        for tree_id in unique_trees:
            tree_mask = (tree_labels == tree_id)
            
            # è¯¥æ ‘æ‰€æœ‰ç‚¹çš„æ ‡å‡†å®ä¾‹ä¸­å¿ƒå‡å€¼
            mean_center_inst = center_inst[tree_mask].mean(dim=0)  # [3]
            
            # è¯¥æ ‘æ‰€æœ‰ç‚¹çš„æ ‘æœ¨å®ä¾‹ä¸­å¿ƒå‡å€¼
            mean_center_tree = center_tree[tree_mask].mean(dim=0)  # [3]
            
            # çº¦æŸï¼šä¸¤ç§ä¸­å¿ƒå‡å€¼åº”è¯¥ä¸€è‡´
            loss = torch.sum((mean_center_inst - mean_center_tree) ** 2)
            total_loss += loss
        
        return total_loss / len(unique_trees)
```

**å…³é”®ç‚¹è§£æ**ï¼š
- çº¦æŸåŒä¸€æ£µæ ‘å†…ï¼Œæœå®/æ ‘å¹²çš„é¢„æµ‹ä¸­å¿ƒå‡å€¼åº”è¯¥å’Œæ ‘æœ¨é¢„æµ‹ä¸­å¿ƒå‡å€¼ä¸€è‡´
- è¿™ä¿è¯äº†å±‚æ¬¡å…³ç³»ï¼šå­å®ä¾‹ï¼ˆæœå®ã€æ ‘å¹²ï¼‰å±äºçˆ¶å®ä¾‹ï¼ˆæ ‘æœ¨ï¼‰

---

## å››ã€æ•°æ®æµå¯¹åº”å…³ç³»

### è®­ç»ƒæ—¶çš„æ•°æ®æµ

```
Ground Truth æ ‡æ³¨
â”œâ”€â”€ positions: [x1, y1, z1], [x2, y2, z2], ...
â”œâ”€â”€ instance:  [  1,   1,   1], [  2,   2], [  3,   3,   3], ...
â””â”€â”€            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                    å®ä¾‹1          å®ä¾‹2          å®ä¾‹3
                       â†“              â†“              â†“
            è®¡ç®—çœŸå®ä¸­å¿ƒ (GT centers)
                c1 = mean(p in ins1)
                c2 = mean(p in ins2)
                c3 = mean(p in ins3)

æ¨¡å‹é¢„æµ‹
â”œâ”€â”€ offsets: [o1, o2, o3], [o4, o5], [o6, o7, o8], ...
â””â”€â”€          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                   â†“             â†“            â†“
           p + offsets â†’ é¢„æµ‹ä¸­å¿ƒ
           [p1+o1, p2+o2, ...] â†’ [c1', c2', ...]

è®­ç»ƒç›®æ ‡
â”œâ”€â”€ å®ä¾‹1çš„ç‚¹: c1' â‰ˆ c1
â”œâ”€â”€ å®ä¾‹2çš„ç‚¹: c2' â‰ˆ c2
â””â”€â”€ å®ä¾‹3çš„ç‚¹: c3' â‰ˆ c3
```

### æ¨ç†æ—¶çš„æ•°æ®æµ

```
æ–°ç‚¹äº‘ï¼ˆæ— æ ‡æ³¨ï¼‰
â”œâ”€â”€ positions: [x1, y1, z1], [x2, y2, z2], ...

æ¨¡å‹é¢„æµ‹
â”œâ”€â”€ offsets: [o1, o2, o3], [o4, o5], ...
â””â”€â”€          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”˜
                   â†“             â†“
           p + offsets â†’ é¢„æµ‹ä¸­å¿ƒ
           [c1', c2', c3', c4', c5', ...]
                   â†“
           HDBSCANèšç±»ï¼ˆspatial clusteringï¼‰
           [c1', c2', c3'] â†’ èšç±»A (å®ä¾‹1)
           [c4', c5']      â†’ èšç±»B (å®ä¾‹2)
                   â†“
           åˆ†é…å®ä¾‹ID
           ç‚¹1,2,3 â†’ å®ä¾‹ID=1
           ç‚¹4,5   â†’ å®ä¾‹ID=2
```

---

## äº”ã€å…³é”®å‚æ•°è¯´æ˜

### HDBSCAN èšç±»å‚æ•°

| å‚æ•° | æ ‡å‡†å®ä¾‹ (offsets1) | æ ‘æœ¨å®ä¾‹ (offsets2) | è¯´æ˜ |
|------|---------------------|---------------------|------|
| `min_cluster_size` | 50 | 200 | æœ€å°èšç±»å¤§å°ï¼šæœå®/æ ‘å¹²è¾ƒå°ï¼Œæ ‘æœ¨è¾ƒå¤§ |
| `min_samples` | 10 | 20 | æ ¸å¿ƒç‚¹çš„æœ€å°é‚»å±…æ•° |

### æŸå¤±æƒé‡

| æŸå¤± | æƒé‡ | è¯´æ˜ |
|------|------|------|
| `L_semantic` | 1.0 | è¯­ä¹‰åˆ†å‰²æŸå¤± |
| `L_instance1` | 1.0 | æ ‡å‡†å®ä¾‹åˆ†å‰²æŸå¤± |
| `L_instance2` | 1.0 | æ ‘æœ¨å®ä¾‹åˆ†å‰²æŸå¤± |
| `L_HCL` | 0.1 | å±‚æ¬¡ä¸€è‡´æ€§æŸå¤± |

---

## å…­ã€ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•æœ‰æ•ˆï¼Ÿ

### âœ… ä¼˜ç‚¹

1. **æ— éœ€å›ºå®šå®ä¾‹æ•°é‡**
   - ä¼ ç»Ÿæ–¹æ³•ï¼šéœ€è¦é¢„å…ˆå®šä¹‰æœ€å¤§å®ä¾‹æ•°ï¼ˆå¦‚100ä¸ªï¼‰
   - åç§»é‡æ–¹æ³•ï¼šèšç±»è‡ªåŠ¨å‘ç°å®ä¾‹æ•°é‡ï¼Œé€‚åº”æ€§æ›´å¼º

2. **å¤„ç†ä»»æ„å½¢çŠ¶å®ä¾‹**
   - ä¸­å¿ƒç‚¹æ–¹æ³•ï¼šå‡è®¾å®ä¾‹æ˜¯çƒå½¢åˆ†å¸ƒ
   - åç§»é‡æ–¹æ³•ï¼šé€šè¿‡å­¦ä¹ åç§»å‘é‡ï¼Œå¯ä»¥å¤„ç†ä¸è§„åˆ™å½¢çŠ¶

3. **è‡ªç„¶çš„å±‚æ¬¡å…³ç³»**
   - åŒå±‚åç§» (offsets1, offsets2) å¯ä»¥ç¼–ç çˆ¶å­å…³ç³»
   - HCLæŸå¤±æ˜¾å¼çº¦æŸå±‚æ¬¡ä¸€è‡´æ€§

### âš ï¸ å±€é™æ€§

1. **ä¾èµ–èšç±»è´¨é‡**
   - HDBSCANå‚æ•°éœ€è¦æ‰‹åŠ¨è°ƒèŠ‚
   - å¯†é›†åœºæ™¯å¯èƒ½è¿‡åº¦èšç±»æˆ–æ¬ èšç±»

2. **è®¡ç®—ä»£ä»·**
   - æ¨ç†æ—¶éœ€è¦é¢å¤–çš„èšç±»æ­¥éª¤
   - å¤§è§„æ¨¡ç‚¹äº‘èšç±»è¾ƒæ…¢

---

## ä¸ƒã€å®é™…æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šé¢„æµ‹ä¸€ä¸ªæœå®

```
Ground Truth (è®­ç»ƒæ•°æ®)
â”œâ”€â”€ ç‚¹1: (1.0, 2.0, 3.0), instance=5  â†’ è¯¥ç‚¹å±äºå®ä¾‹5
â”œâ”€â”€ ç‚¹2: (1.1, 2.1, 3.0), instance=5  â†’ è¯¥ç‚¹å±äºå®ä¾‹5
â””â”€â”€ ç‚¹3: (1.2, 2.0, 2.9), instance=5  â†’ è¯¥ç‚¹å±äºå®ä¾‹5

è®¡ç®—å®ä¾‹5çš„çœŸå®ä¸­å¿ƒ:
c_gt = mean([(1.0,2.0,3.0), (1.1,2.1,3.0), (1.2,2.0,2.9)])
     = (1.1, 2.03, 2.97)

è®­ç»ƒç›®æ ‡:
â”œâ”€â”€ ç‚¹1é¢„æµ‹åç§»: o1 ä½¿å¾— (1.0,2.0,3.0) + o1 â‰ˆ (1.1,2.03,2.97)
â”‚                    â†’ o1 â‰ˆ (0.1, 0.03, -0.03)
â”œâ”€â”€ ç‚¹2é¢„æµ‹åç§»: o2 ä½¿å¾— (1.1,2.1,3.0) + o2 â‰ˆ (1.1,2.03,2.97)
â”‚                    â†’ o2 â‰ˆ (0.0, -0.07, -0.03)
â””â”€â”€ ç‚¹3é¢„æµ‹åç§»: o3 ä½¿å¾— (1.2,2.0,2.9) + o3 â‰ˆ (1.1,2.03,2.97)
                     â†’ o3 â‰ˆ (-0.1, 0.03, 0.07)

æ¨ç†æ—¶ (æ–°æ•°æ®æ— æ ‡æ³¨):
â”œâ”€â”€ æ–°ç‚¹1: (0.8, 1.9, 3.1) + o1_pred = (0.9, 1.95, 3.05)
â”œâ”€â”€ æ–°ç‚¹2: (0.9, 2.0, 3.0) + o2_pred = (0.92, 1.98, 3.02)
â””â”€â”€ æ–°ç‚¹3: (1.0, 2.1, 3.1) + o3_pred = (0.95, 2.00, 3.08)

HDBSCANèšç±»:
â”œâ”€â”€ é¢„æµ‹ä¸­å¿ƒ: [(0.9,1.95,3.05), (0.92,1.98,3.02), (0.95,2.00,3.08)]
â””â”€â”€ èšç±»ç»“æœ: è¿™3ä¸ªä¸­å¿ƒè·ç¦»å¾ˆè¿‘ â†’ åŒä¸€èšç±» â†’ å®ä¾‹ID=1
```

---

## å…«ã€æ€»ç»“

### æ ¸å¿ƒæµç¨‹
```
è®­ç»ƒ (train):     GTæ ‡ç­¾ â†’ è®¡ç®—çœŸå®ä¸­å¿ƒ â†’ è®­ç»ƒåç§»é¢„æµ‹ â†’ ä¼˜åŒ–æ¨¡å‹
                  âŒ ä¸æ‰§è¡Œèšç±»ï¼ˆåªç”¨GTæ ‡ç­¾è®¡ç®—æŸå¤±ï¼‰

éªŒè¯ (val):       ç‚¹äº‘ â†’ é¢„æµ‹åç§» â†’ è®¡ç®—ä¸­å¿ƒ â†’ HDBSCANèšç±» â†’ å®ä¾‹ID â†’ PQæŒ‡æ ‡
                  âš ï¸ æœ‰æ¡ä»¶æ‰§è¡Œï¼ˆepoch > pq_from_epoch æ—¶æ‰èšç±»ï¼‰

æµ‹è¯• (test):      ç‚¹äº‘ â†’ è°ƒç”¨é˜¶æ®µ |
|------|----------|---------|---------|
| æ¨¡å‹å®šä¹‰ | `models/hapt3d_ours.py` | `HAPT3D.forward()` | train/val/test |
| å®ä¾‹æŸå¤± | `utils/lovasz.py` | `IoULovaszLoss` | train/val/test |
| HCLæŸå¤± | `utils/hcl_loss.py` | `HierarchicalConsistencyLoss` | train/val/test |
| **èšç±»åå¤„ç†** | `models/hapt3d_ours.py` | `post_processing()` | **val/test** |
| æ‰‹åŠ¨å¯¼å‡º | `export_ply.py` | `model_inference()` | è‡ªå®šä¹‰
```

### å„é˜¶æ®µçš„åå¤„ç†å¯¹æ¯”

| é˜¶æ®µ | æ˜¯å¦èšç±» | è°ƒç”¨ä½ç½® | ç›®çš„ |
|------|---------|---------|------|
| **è®­ç»ƒ (train)** | âŒ å¦ | - | ä»…è®¡ç®—æŸå¤±ä¼˜åŒ–æ¨¡å‹ |
| **éªŒè¯ (val)** | âš ï¸ æ¡ä»¶æ‰§è¡Œ | `step() â†’ post_processing()` | è®¡ç®—PQæŒ‡æ ‡ç›‘æ§è®­ç»ƒ |
| **æµ‹è¯• (test)** | âœ… æ˜¯ | `step() â†’ post_processing()` | å®Œæ•´è¯„ä¼°æ¨¡å‹æ€§èƒ½ |
| **å¯¼å‡º (export)** | âœ… æ˜¯ | `export_ply.py â†’ model_inference()` | å¯è§†åŒ–åˆ†æ |

### å…³é”®ä»£ç ä½ç½®

| åŠŸèƒ½ | æ–‡ä»¶è·¯å¾„ | å‡½æ•°/ç±» |
|------|----------|---------|
| æ¨¡å‹å®šä¹‰ | `models/hapt3d_ours.py` | `HAPT3D.forward()` |
| å®ä¾‹æŸå¤± | `utils/lovasz.py` | `IoULovaszLoss` |
| HCLæŸå¤± | `utils/hcl_loss.py` | `HierarchicalConsistencyLoss` |
| æ¨ç†èšç±» | `export_ply.py` | `model_inference()` |

### å‚è€ƒèµ„æ–™

- **ASIS (CVPR 2019)**: é¦–æ¬¡æå‡ºoffset-basedæ–¹æ³•
- **HAIS (ICCV 2021)**: æ”¹è¿›çš„èšç±»ç­–ç•¥
- **SoftGroup (CVPR 2022)**: è½¯åˆ†ç»„èšåˆ
- **æœ¬æ–‡æ–¹æ³• (HAPT3D)**: åŒå±‚åç§» + HCLå±‚æ¬¡çº¦æŸ

---

**ğŸ“ å»ºè®®é˜…è¯»é¡ºåº**ï¼š
1. å…ˆçœ‹"å®Œæ•´æµç¨‹å›¾è§£"äº†è§£æ•´ä½“
2. å†çœ‹"æ ¸å¿ƒä»£ç è§£æ"ç†è§£å®ç°
3. æœ€åçœ‹"å®é™…æ¡ˆä¾‹"å·©å›ºç†è§£
